{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f7454ea-d2a6-41ba-b1f5-f9c35668fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras \n",
    "import hazm\n",
    "from keras.models import load_model\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa3eba-9263-408a-8df6-abce6575488c",
   "metadata": {},
   "source": [
    "# **Text Preprocessing and Data Preparation for Machine Learning Models Using Hazm**\n",
    "\n",
    "This code preprocesses Persian textual data and prepares it for machine learning models, such as RNNs or LSTMs.\n",
    "\n",
    "---\n",
    "\n",
    "## **Code Overview**\n",
    "\n",
    "The following steps are performed:\n",
    "\n",
    "1. **Reading Data from a File**  \n",
    "   The text is read from `data.txt` and stored in a variable.\n",
    "\n",
    "2. **Removing Empty Lines**  \n",
    "   Extra empty lines (`\\n\\n`) are removed from the text.\n",
    "\n",
    "3. **Stemming and Lemmatization**  \n",
    "   The `hazm` library is used to normalize the text:\n",
    "   - **Stemming:** Reduces words to their root form.\n",
    "   - **Lemmatization:** Converts words to their base dictionary form.\n",
    "\n",
    "4. **Tokenization and POS Tagging**  \n",
    "   - The text is tokenized into individual words.  \n",
    "   - POS (Part-of-Speech) tagging is applied using the `pos_tagger.model`.  \n",
    "   - Each word is tagged in the format `[word-POS_tag]`.\n",
    "\n",
    "5. **Creating Unique Tokens and Counting Frequencies**  \n",
    "   - A list of unique tokens is created.  \n",
    "   - The frequency of each token in the text is calculated.\n",
    "\n",
    "6. **Token-to-Index Conversion**  \n",
    "   Tokens are replaced with their respective indices to prepare numerical input for models.\n",
    "\n",
    "7. **Data Preparation for Model Training**  \n",
    "   A sliding window of size 10 is applied:\n",
    "   - `X_train` contains sequences of 10 consecutive words (as indices).  \n",
    "   - `Y_train` contains the next word following each sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6351355d-e60d-4226-bd37-9cac59a0c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.txt\",\"r\") as file:\n",
    "    contact =file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "902c92eb-de65-4567-b3b3-fdc8a3181528",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"زن آه کشید و گفت: «همه چیز به این قضیه مربوط است!»\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e66c6703-af31-43b4-9455-23858023edf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "contact = contact.replace(\"\\n\\n\",\"\")\n",
    "text = text.replace(\"\\n\\n\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3089ae6e-6e0f-4b86-9117-469120a060f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = hazm.Stemmer()\n",
    "contact=stemmer.stem(contact)\n",
    "text=stemmer.stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3de441bc-3fc5-462f-9dff-34d9d45bf128",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = hazm.Lemmatizer()\n",
    "contact=lemmatizer.lemmatize(contact)\n",
    "text=lemmatizer.lemmatize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "397c9cd3-06a5-414d-b443-4b85ebef41f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge (arr) :\n",
    "    \n",
    "    arr1=[]\n",
    "    for v in arr:\n",
    "        arr1.append(f\"[{v[0]}-{v[1]}]\")\n",
    "    return arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5062e723-ca0d-45ac-9e2e-cf3837b9712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "contact=hazm.word_tokenize(contact)\n",
    "text=hazm.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4598317e-328f-4920-b456-4349f5ab8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_posTagger = hazm.POSTagger(model='pos_tagger.model')\n",
    "contact=merge(spacy_posTagger.tag(tokens = contact))\n",
    "text=merge(spacy_posTagger.tag(tokens = text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1590710-6512-4e2c-885d-2b8ffae975c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = []\n",
    "count={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d72bca8-5ce8-4448-a982-0aa6e054a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,value in enumerate(contact):\n",
    "    if not value in token:\n",
    "        token.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52c2a449-1270-4c59-b831-dddfa0dbd853",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,value in enumerate(token):\n",
    "    count[value]=contact.count(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57430fb8-49d4-4e21-b3c5-e2d315c32961",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,value in enumerate(text):\n",
    "    text[index] = token.index(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f07dbd9b-6039-4185-aecf-fbe6108e5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[]\n",
    "Y_train=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb426fff-3052-442a-97d0-e4c620efed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,value in enumerate(text):\n",
    "    if len(text)-10 > index:\n",
    "        X_train.append(text[index:index+10])\n",
    "        Y_train.append(text[index+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3cf6fea-ad84-4172-901b-9c036e1697eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3422fe51-fc8a-4337-9e0d-0f80828a0326",
   "metadata": {},
   "source": [
    "# **Text Generation Using Trained LSTM Model**\n",
    "\n",
    "This code uses a trained LSTM model to generate text. The model predicts the next words based on previously learned data.\n",
    "\n",
    "---\n",
    "\n",
    "## **Code Overview**\n",
    "\n",
    "### **Code Functionality:**\n",
    "1. **Loading the Model:** The saved model `token_generator_best_loss.keras` is loaded.  \n",
    "2. **Array to Text Conversion:** The function `arr_to_text` converts an array of tokens into text.  \n",
    "3. **Text Cleaning:** The function `clean_text` cleans the generated text by removing unnecessary characters (English letters and symbols).  \n",
    "4. **Generating Next Words:**\n",
    "   - The model predicts the next word and updates the sequence accordingly.  \n",
    "   - The predicted word is appended to the sequence, and the first word is removed.  \n",
    "5. **Output Display:** The generated text is printed at each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24f9dbe8-edef-4bde-914a-72ce2deb9b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"best_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "842066dc-ba71-4720-9f05-b48f8ca8acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arr_to_text(sequence):\n",
    "    text = \"\"\n",
    "    for token_index in sequence:\n",
    "        text += \" \" + token[int(token_index)]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d39594e8-159d-4c03-ab84-51d9e2304ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sequence, output):\n",
    "    generated_text = arr_to_text(sequence) + \" \" + token[np.argmax(output)]\n",
    "    generated_text = re.sub(r'[a-zA-Z]+', '', generated_text) \n",
    "    generated_text = re.sub(r'[\\[\\]-]', '', generated_text) \n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b089a219-5331-42c1-aa0c-9c84255391fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_sequence = X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf19af33-75d9-4df6-a509-6db1ec73c6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    input_sequence = np.array([current_sequence]) \n",
    "    predicted_output = model.predict(input_sequence) \n",
    "    print(clean_text(current_sequence, predicted_output)) \n",
    "    \n",
    "    # به‌روزرسانی توالی برای پیش‌بینی بعدی\n",
    "    current_sequence = np.delete(current_sequence, 0) \n",
    "    current_sequence = np.append(current_sequence, np.argmax(predicted_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
